{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bcaeb0",
   "metadata": {},
   "source": [
    "First we need to import the Libraries and establish connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac18b762-b987-4b6f-bd39-e8b9a97438ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6426c3ee-3b02-4718-ad5e-ed9665e19052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': '',\n",
    "    'port': '',\n",
    "    'database': 'postgres',\n",
    "    'user': '',\n",
    "    'password': ''\n",
    "}\n",
    "\n",
    "# Function to establish a database connection\n",
    "def get_db_connection(db_params):\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=db_params['host'],\n",
    "            port=db_params['port'],\n",
    "            database=db_params['database'],\n",
    "            user=db_params['user'],\n",
    "            password=db_params['password']\n",
    "        )\n",
    "        return conn\n",
    "    except psycopg2.DatabaseError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to retrieve data from a specific table\n",
    "def get_table_data(db_params, table_name):\n",
    "    conn = get_db_connection(db_params)\n",
    "    if conn is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f930d145-4fd2-4ad8-8ab7-08239f0a4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve data from a specific table\n",
    "def get_table_data(db_params, table_name):\n",
    "    conn = get_db_connection(db_params)\n",
    "    if conn is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3a1db",
   "metadata": {},
   "source": [
    "Then we need to load the dataset after it was cleaned in the pre-processing section (ILO5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee33a2da-d9ff-414e-a396-94a96928821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accident severity First Mode of Transport           Area Type  \\\n",
      "0               0.0      -0.350029334776985  0.3316993365651348   \n",
      "1               0.0      -1.810821489630903  0.3316993365651348   \n",
      "2               0.0      -0.350029334776985  0.3316993365651348   \n",
      "3               0.0      -0.350029334776985  0.3316993365651348   \n",
      "4               0.0      -0.350029334776985   -3.01477841455867   \n",
      "\n",
      "      Light condition        Road Location       Road condition  \\\n",
      "0  0.6027962867863099  -0.8738398047923384    1.513226708878669   \n",
      "1  0.6027962867863099   1.1443745117992685  -0.6608395121052414   \n",
      "2  0.6027962867863099  -0.8738398047923384  -0.6608395121052414   \n",
      "3  0.6027962867863099   1.1443745117992685  -0.6608395121052414   \n",
      "4  -1.658935235535878   1.1443745117992685  -0.6608395121052414   \n",
      "\n",
      "          Road surface        Road situation          Speed limit  \\\n",
      "0    1.784021822931772  -0.42384880344171616   1.9706356443111297   \n",
      "1  -0.7354366365307541    1.0394771034977446  -0.1099889661941095   \n",
      "2  0.10438284995675458  -0.42384880344171616  -0.1099889661941095   \n",
      "3  -0.7354366365307541    1.0394771034977446  -0.1099889661941095   \n",
      "4   0.9442023364442633    1.0394771034977446   3.0109479495637492   \n",
      "\n",
      "               weather accidents       avg_longitude        avg_latitude  \n",
      "0   2.4184991070981363       1.0   4.735870517806607   51.60179951753548  \n",
      "1  -0.4134795820536239       1.0  4.8004909468402825   51.58323783100281  \n",
      "2  -0.4134795820536239       1.0   4.810410549597804   51.57978725868066  \n",
      "3  -0.4134795820536239       1.0   4.774711355162457   51.61065052547654  \n",
      "4  -0.4134795820536239       1.0   4.737351544800915  51.562642063151664  \n",
      "  Accident severity First Mode of Transport           Area Type  \\\n",
      "0               0.0      -0.350029334776985  0.3316993365651348   \n",
      "1               0.0      -0.350029334776985  0.3316993365651348   \n",
      "2               0.0      -0.350029334776985  0.3316993365651348   \n",
      "3               0.0      -0.350029334776985  0.3316993365651348   \n",
      "4               0.0      -0.350029334776985   -3.01477841455867   \n",
      "\n",
      "      Light condition        Road Location       Road condition  \\\n",
      "0  -1.658935235535878  -0.8738398047923384  -0.6608395121052414   \n",
      "1  -1.658935235535878  -0.8738398047923384  -0.6608395121052414   \n",
      "2  -1.658935235535878   1.1443745117992685  -0.6608395121052414   \n",
      "3  -1.658935235535878  -0.8738398047923384  -0.6608395121052414   \n",
      "4  -1.658935235535878   1.1443745117992685  -0.6608395121052414   \n",
      "\n",
      "          Road surface        Road situation          Speed limit  \\\n",
      "0  -0.7354366365307541  -0.42384880344171616  -0.1099889661941095   \n",
      "1  -0.7354366365307541   -1.1555117569114464  -0.1099889661941095   \n",
      "2  -0.7354366365307541    1.0394771034977446   1.9706356443111297   \n",
      "3  -0.7354366365307541  -0.42384880344171616   1.9706356443111297   \n",
      "4   0.9442023364442633    1.0394771034977446   3.0109479495637492   \n",
      "\n",
      "               weather accidents      avg_longitude        avg_latitude  \n",
      "0  -0.4134795820536239       1.0  4.810779114041805  51.573813778757604  \n",
      "1  -0.4134795820536239       1.0  4.765561273223476  51.590071305296476  \n",
      "2  -0.4134795820536239       1.0  4.744535951576343     51.593685627824  \n",
      "3  -0.4134795820536239       1.0  4.737351544800915  51.562642063151664  \n",
      "4  -0.4134795820536239       1.0  4.737351544800915  51.562642063151664  \n",
      "  Accident severity First Mode of Transport           Area Type  \\\n",
      "0               0.0      -1.810821489630903   -3.01477841455867   \n",
      "1               0.0      -0.350029334776985  0.3316993365651348   \n",
      "2               0.0     0.38036674264997405  0.3316993365651348   \n",
      "3               0.0        3.30195105235781  0.3316993365651348   \n",
      "4               0.0      -0.350029334776985  0.3316993365651348   \n",
      "\n",
      "      Light condition        Road Location       Road condition  \\\n",
      "0  0.6027962867863099   1.1443745117992685  -0.6608395121052414   \n",
      "1  -1.658935235535878   1.1443745117992685  -0.6608395121052414   \n",
      "2  0.6027962867863099  -0.8738398047923384  -0.6608395121052414   \n",
      "3  -1.658935235535878  -0.8738398047923384    1.513226708878669   \n",
      "4  -1.658935235535878  -0.8738398047923384  -0.6608395121052414   \n",
      "\n",
      "          Road surface        Road situation          Speed limit  \\\n",
      "0  -0.7354366365307541    1.0394771034977446   0.9303233390585102   \n",
      "1  -0.7354366365307541    1.0394771034977446  -0.1099889661941095   \n",
      "2  -0.7354366365307541  -0.42384880344171616  -0.1099889661941095   \n",
      "3  -0.7354366365307541   0.30781415002801416  -0.1099889661941095   \n",
      "4  -0.7354366365307541    1.0394771034977446  -0.1099889661941095   \n",
      "\n",
      "               weather accidents       avg_longitude        avg_latitude  \n",
      "0  -0.4134795820536239       1.0   4.790611555409986   51.55755331349928  \n",
      "1  -0.4134795820536239       1.0  4.8004909468402825   51.58323783100281  \n",
      "2  -0.4134795820536239       1.0   4.737748620492085  51.587281541800316  \n",
      "3  -0.4134795820536239       1.0   4.790685690276276   51.60055204331606  \n",
      "4  -0.4134795820536239       1.0   4.754905564166104   51.59356679160258  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_583855/1785455050.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Load the training, testing, and validation datasets from the database\n",
    "train_table_name = 'group12_warehouse.train_table'\n",
    "test_table_name = 'group12_warehouse.test_table'\n",
    "validation_table_name = 'group12_warehouse.validation_table'\n",
    "\n",
    "train_data = get_table_data(db_params, train_table_name)\n",
    "test_data = get_table_data(db_params, test_table_name)\n",
    "validation_data = get_table_data(db_params, validation_table_name)\n",
    "\n",
    "if train_data is not None and test_data is not None and validation_data is not None:\n",
    "    print(train_data.head())\n",
    "    print(test_data.head())\n",
    "    print(validation_data.head())\n",
    "else:\n",
    "    print(\"Failed to retrieve data from one or more tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038ffef",
   "metadata": {},
   "source": [
    "Then the data is split to prepare datasets for a machine learning task by separating the features (independent variables) from the target variable (dependent variable) related to accident severity. The train_data, test_data, and validation_data datasets each contain various features along with the 'Accident severity' column, which indicates the severity of an accident. The code first removes the 'Accident severity' column from each dataset to create feature sets (X_train, X_test, and X_val). These feature sets now contain all the data except for the target variable. Next, it extracts the 'Accident severity' column and stores it separately as target variables (y_train, y_test, and y_val). This separation is crucial because it allows machine learning models to learn from the features (X_train), make predictions, and then compare those predictions against the actual outcomes (y_train) to evaluate performance. The test and validation sets serve similar purposes, providing data for assessing the model's generalization capability and fine-tuning, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7990ee30-d50d-4e0b-bb84-69fb3528c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target variable\n",
    "X_train = train_data.drop(columns=['Accident severity'])\n",
    "y_train = train_data[['Accident severity']]\n",
    "    \n",
    "X_test = test_data.drop(columns=['Accident severity'])\n",
    "y_test = test_data[['Accident severity']]\n",
    "    \n",
    "X_val = validation_data.drop(columns=['Accident severity'])\n",
    "y_val = validation_data[['Accident severity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958388ff-2ad4-4eb9-ae54-15144d53d14a",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810ececc-557b-4ff7-8cf9-4a378770700e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       251\n",
      "         1.0       0.78      0.98      0.87       251\n",
      "         2.0       0.97      0.69      0.80       251\n",
      "\n",
      "    accuracy                           0.89       753\n",
      "   macro avg       0.90      0.89      0.88       753\n",
      "weighted avg       0.90      0.89      0.88       753\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       508\n",
      "         1.0       0.76      0.96      0.85       508\n",
      "         2.0       0.95      0.68      0.79       508\n",
      "\n",
      "    accuracy                           0.88      1524\n",
      "   macro avg       0.90      0.88      0.88      1524\n",
      "weighted avg       0.90      0.88      0.88      1524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Generate the classification report for the test set\n",
    "report_test = classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred_val = clf.predict(X_val)\n",
    "\n",
    "# Generate the classification report for the validation set\n",
    "report_val = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(report_test)\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report for Validation Set:\")\n",
    "print(report_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382628f",
   "metadata": {},
   "source": [
    "The classification report for our decision tree model provides a detailed evaluation of its performance on both the test and validation datasets. The model achieved an overall accuracy of 89% on the test set and 88% on the validation set, indicating a strong predictive ability. \n",
    "\n",
    "For the test set, class 0 had an impressive precision and recall of 0.97 and 1.00, respectively, resulting in an F1-score of 0.98. Class 1 showed lower precision at 0.78 but high recall at 0.98, giving an F1-score of 0.87. Class 2 had a high precision of 0.97 but lower recall at 0.69, leading to an F1-score of 0.80. \n",
    "\n",
    "Similarly, in the validation set, class 0 maintained excellent precision and recall, both at 0.98 and 1.00 respectively, with an F1-score of 0.99. Class 1 had a precision of 0.76 and recall of 0.96, resulting in an F1-score of 0.85. Class 2 had a precision of 0.95 and recall of 0.68, leading to an F1-score of 0.79. \n",
    "\n",
    "Overall, the model demonstrates high effectiveness, particularly in identifying class 0 across both datasets. However, it shows some variability in precision and recall for classes 1 and 2, indicating areas for potential improvement. The macro and weighted averages confirm consistent performance across different metrics and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189b20c-2a6b-4009-9e96-4edaebf1fdfe",
   "metadata": {},
   "source": [
    "## Hypertuning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1b7142-60e1-416c-8e13-7e75a29e9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters found by GridSearchCV:\n",
      "{'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "\n",
      "Classification Report for Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       251\n",
      "         1.0       0.78      0.98      0.87       251\n",
      "         2.0       0.97      0.69      0.80       251\n",
      "\n",
      "    accuracy                           0.89       753\n",
      "   macro avg       0.90      0.89      0.88       753\n",
      "weighted avg       0.90      0.89      0.88       753\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       508\n",
      "         1.0       0.78      0.96      0.86       508\n",
      "         2.0       0.95      0.72      0.82       508\n",
      "\n",
      "    accuracy                           0.89      1524\n",
      "   macro avg       0.90      0.89      0.89      1524\n",
      "weighted avg       0.90      0.89      0.89      1524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred_test = best_clf.predict(X_test)\n",
    "\n",
    "# Generate the classification report for the test set\n",
    "report_test = classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred_val = best_clf.predict(X_val)\n",
    "\n",
    "# Generate the classification report for the validation set\n",
    "report_val = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print(\"Best Parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(report_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Classification Report for Validation Set:\")\n",
    "print(report_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367343f",
   "metadata": {},
   "source": [
    "Best Parameters found for the decision tree model are: {'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "\n",
    "The decision tree model was trained using specific parameters to optimize its performance. The 'criterion' parameter was set to 'entropy,' which means the model uses entropy to measure the quality of splits at each node. This approach aims to reduce uncertainty and create more homogeneous branches. The 'max_depth' parameter was assigned a value of 40, indicating that the tree can have up to 40 levels. This allows the model to capture complex patterns but risks overfitting if not properly managed. The 'min_samples_leaf' parameter was set to 1, meaning that a leaf node must have at least one sample. This setting ensures the tree can continue growing until all leaves are pure or contain a single sample. Finally, the 'min_samples_split' parameter was set to 2, which specifies that a node must have at least two samples to consider splitting further. These parameters collectively influence how the tree is structured and how it makes decisions based on the training data. By fine-tuning these parameters, the model aims to balance complexity and performance, capturing the underlying patterns in the data while avoiding overfitting.\n",
    "\n",
    "the hypertuning has only incresed the accuracy by 1% which consdering it's initially high accuracy was not unexpected as there was little room for improvment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
